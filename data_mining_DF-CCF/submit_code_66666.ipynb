{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0-读取csv数据，存成numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load_from_path(filePath):\n",
    "    feat_line = 0\n",
    "    cur_feature_list = []\n",
    "    with open(filePath,'r') as f:\n",
    "        cur_file = csv.reader(f)\n",
    "        for cur_feature in cur_file:\n",
    "            if feat_line == 0:\n",
    "                feat_line += 1\n",
    "                continue\n",
    "            cur_feature_line = []\n",
    "            for f_data in cur_feature:\n",
    "                cur_feature_line.append(float(f_data))\n",
    "            # print(len(cur_feature_line))\n",
    "            cur_feature_list.append(cur_feature_line)\n",
    "            feat_line += 1\n",
    "    return cur_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(file_path,file_name_list):\n",
    "    ff = open(file_path,'w')\n",
    "    for ii in file_name_list:\n",
    "        ff.write(ii + \"\\n\")\n",
    "    ff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    ff = open(file_path)\n",
    "    file_name = []\n",
    "    for ii in ff:\n",
    "        file_name.append(ii.strip())\n",
    "    ff.close()\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_union(total_feature_list):\n",
    "    maxLen = 0\n",
    "    minLen = 2100000\n",
    "    diffLenNumer = {}\n",
    "    for data_array in total_feature_list:\n",
    "        cur_len = len(data_array)\n",
    "        if cur_len < minLen:\n",
    "            minLen = cur_len\n",
    "        if cur_len > maxLen:\n",
    "            maxLen = cur_len\n",
    "        if diffLenNumer.has_key(cur_len) == False:\n",
    "            diffLenNumer[cur_len] = 1\n",
    "        else:\n",
    "            diffLenNumer[cur_len] = diffLenNumer[cur_len] + 1\n",
    "    #for key in diffLenNumer.keys():\n",
    "        #print(key,\" : \",diffLenNumer[key])\n",
    "    print('minLen:',minLen,\" maxLen:\",maxLen)\n",
    "    maxLen = maxLen + 1\n",
    "    data_numpy = np.zeros((len(total_feature_list),maxLen,len(total_feature_list[0][0])))\n",
    "    for index,data_array in enumerate(total_feature_list):\n",
    "        cur_array = np.array(data_array)\n",
    "        data_numpy[index,maxLen - len(cur_array):,:] = cur_array[:,:]\n",
    "        data_numpy[index][0][0] = maxLen - len(cur_array)\n",
    "    print(data_numpy.shape)\n",
    "    return data_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpFile = r'./tmpFile/'\n",
    "def data_load(trainPath,trainLabelpath,testPath=None):\n",
    "    csv_file = csv.reader(open(trainLabelpath,'r'))\n",
    "    cnt = 0\n",
    "    labelList = []\n",
    "    total_feature_list = []\n",
    "    for stu in csv_file:\n",
    "        if cnt % 1000 == 2:\n",
    "            print(cnt)\n",
    "        if cnt == 0:\n",
    "            cnt += 1\n",
    "            continue\n",
    "        cur_label = int(stu[2])\n",
    "        cur_path = trainPath + stu[0] + '/' + stu[1]\n",
    "        if os.path.exists(cur_path):\n",
    "            cur_feature_list = data_load_from_path(cur_path)\n",
    "            total_feature_list.append(cur_feature_list)\n",
    "            labelList.append(cur_label)\n",
    "        cnt += 1\n",
    "    labelList = np.array(labelList)\n",
    "    trainData = data_union(total_feature_list)\n",
    "    total_feature_list_test = []\n",
    "    testFileName = []\n",
    "    for index,cur_path in enumerate(os.listdir(testPath)):\n",
    "        if index % 1000 == 2:\n",
    "            print(index)\n",
    "        testFileName.append(cur_path)\n",
    "        cur_feature_list = data_load_from_path(testPath + cur_path)\n",
    "        total_feature_list_test.append(cur_feature_list)\n",
    "    testData = data_union(total_feature_list_test)\n",
    "    if os.path.exists(tmpFile) == False:\n",
    "        os.mkdir(tmpFile)\n",
    "    np.save(tmpFile + 'trainData.npy',trainData)\n",
    "    np.save(tmpFile + 'labelList.npy',labelList)\n",
    "    np.save(tmpFile + 'testData.npy',testData)\n",
    "    write_file(tmpFile + 'test_name.txt',testFileName)\n",
    "    return trainData,labelList,testData,testFileName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据，这部分建议在服务器上跑，以免出现内存或者硬盘空间不足的情况。numpy文件将保存到tmpFile目录下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1002\n",
      "2002\n",
      "3002\n",
      "4002\n",
      "5002\n",
      "6002\n",
      "7002\n",
      "8002\n",
      "9002\n",
      "10002\n",
      "11002\n",
      "12002\n",
      "13002\n",
      "14002\n",
      "15002\n",
      "16002\n",
      "17002\n",
      "18002\n",
      "19002\n",
      "20002\n",
      "21002\n",
      "22002\n",
      "23002\n",
      "24002\n",
      "25002\n",
      "26002\n"
     ]
    }
   ],
   "source": [
    "rootpath = r'./'\n",
    "trainPath = rootpath + 'train/'\n",
    "testPath = rootpath + 'test/'\n",
    "labelPath = rootpath + 'train_labels.csv'\n",
    "trainX,trainY,testX,testFileName = data_load(trainPath,labelPath,testPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = np.load(tmpFile + 'trainData.npy')\n",
    "labelList = np.load(tmpFile + 'labelList.npy')\n",
    "testData = np.load(tmpFile + 'testData.npy')\n",
    "testFileName = read_file(tmpFile + 'test_name.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48339, 451, 75) (48339,) (87498, 451, 75) 87498\n"
     ]
    }
   ],
   "source": [
    "print trainData.shape,labelList.shape,testData.shape,len(testFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除最后三列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48339, 451, 72) (87498, 451, 72)\n"
     ]
    }
   ],
   "source": [
    "trainData = trainData[:,:,:-3]\n",
    "testData = testData[:,:,:-3]\n",
    "print trainData.shape,testData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理，删除轮毂转速（index-0）为0且超速传感器转速检测值（indx-8）为0的行。如果test-data的当前文件全部为0，则直接判定为0，并保存到delete-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_speed0(data,labelList=None,data_name=None,train_flag=True):\n",
    "    maxLen = len(data[0])\n",
    "    factor_number = len(data[0][0])\n",
    "    clean_data_item0 = []\n",
    "    clean_y = []\n",
    "    clean_name = []\n",
    "    delete_index = []\n",
    "    for ii in range(len(data)):\n",
    "        st = int(data[ii][0][0])\n",
    "        cur_data = data[ii,st:]\n",
    "        speed_0 = np.where((cur_data[:,0] == 0) & (cur_data[:,8] == 0))[0]  ##speed 0 is deleted\n",
    "        cur_data = np.delete(cur_data, speed_0, 0)\n",
    "        if len(cur_data) < 1:\n",
    "            delete_index.append(ii)\n",
    "            continue\n",
    "        clean_item0_equal_0 = np.zeros((maxLen,factor_number))\n",
    "        new_st = maxLen - len(cur_data)\n",
    "        clean_item0_equal_0[new_st:,:] = cur_data\n",
    "        clean_item0_equal_0[0][0] = new_st\n",
    "        clean_data_item0.append(clean_item0_equal_0)\n",
    "        if train_flag:\n",
    "            clean_y.append(labelList[ii])\n",
    "        else:\n",
    "            clean_name.append(data_name[ii])\n",
    "    clean_data_item0_array = np.array(clean_data_item0)\n",
    "    #print clean_data_item0_array.shape\n",
    "    return  clean_data_item0_array,np.array(clean_y),clean_name,delete_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_data,clean_train_y,_,train_delete_index = clean_speed0(trainData,labelList=labelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48339, 451, 72) (46620, 451, 72) 46620 1719\n"
     ]
    }
   ],
   "source": [
    "print trainData.shape,clean_train_data.shape,len(clean_train_y),len(train_delete_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 避免内存占用过度，删除trainData\n",
    "del trainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_data,_,clean_test_name,test_delete_index = clean_speed0(testData,data_name=testFileName,train_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87498, 451, 72) (87139, 451, 72) 87139 359\n"
     ]
    }
   ],
   "source": [
    "print testData.shape,clean_test_data.shape,len(clean_test_name),len(test_delete_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 避免内存占用过度，删除testData\n",
    "del testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(tmpFile + 'clean_train_data.npy',clean_train_data)\n",
    "np.save(tmpFile + 'clean_train_y.npy',clean_train_y)\n",
    "np.save(tmpFile + 'clean_test_data.npy',clean_test_data)\n",
    "write_file(tmpFile + 'clean_test_name.txt',clean_test_name)\n",
    "np.save(tmpFile + 'delete_index.npy',test_delete_index)  ##test_index should be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stage-1 整体--数据提取特征和F1指标训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_mean_and_std(data_array):\n",
    "    if len(data_array.shape) == 1:\n",
    "        data_array = data_array.reshape(-1,1)\n",
    "    feature_time_and_fft = []\n",
    "    feature_time_and_fft.extend(np.mean(data_array,axis=0))\n",
    "    feature_time_and_fft.extend(np.std(data_array,axis=0))\n",
    "    return feature_time_and_fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征1——提取所有变量的均值和方差，共计72 * 2 = 144维特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(data):\n",
    "    item0_fea = []\n",
    "    for ii in range(len(data)):\n",
    "        st = int(data[ii][0][0])\n",
    "        cur_data = data[ii,st:]\n",
    "        item0_fea.append(feature_mean_and_std(cur_data))\n",
    "    item0_ff = np.array(item0_fea)\n",
    "    return item0_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = get_feature(clean_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = get_feature(clean_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46620, 144) (46620,) (87139, 144) 87139\n"
     ]
    }
   ],
   "source": [
    "print trainX.shape,clean_train_y.shape,testX.shape,len(clean_test_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征2——对index-65提取特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_item65(data):\n",
    "    data65 = data[:,-100:,65]\n",
    "    feature = []\n",
    "    for ii in range(len(data65)):\n",
    "        pt = np.where(data65[ii] > 100)[0]\n",
    "        if len(pt) > 0:\n",
    "            feature.append(1)\n",
    "        else:\n",
    "            feature.append(0)\n",
    "    feature = np.array(feature)\n",
    "    #print feature.shape\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fea_item65 = feature_item65(clean_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32683 13937\n"
     ]
    }
   ],
   "source": [
    "print len(np.where(train_fea_item65 == 0)[0]),len(np.where(train_fea_item65 == 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fea_item65 = feature_item65(clean_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46620,) (87139,)\n"
     ]
    }
   ],
   "source": [
    "print train_fea_item65.shape,test_fea_item65.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76287 10852\n"
     ]
    }
   ],
   "source": [
    "print len(np.where(test_fea_item65 == 0)[0]),len(np.where(test_fea_item65 == 1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过上述对index-65的划分，我们可以看到train集合和test集合是有差别的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征3——对下面的数据进行处理，避免左偏，右偏现象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = 451\n",
    "feature_number = 2\n",
    "def new_item_index(data):  ##power(x,0.5)\n",
    "    power_index = (13,19,20,45,46,21,23)\n",
    "    new_item_index_feature = []\n",
    "    for ii in range(len(data)):\n",
    "        st = int(data[ii][0][0])\n",
    "        cur_data = data[ii,st:,:]\n",
    "        cur_data_fea = []\n",
    "        for jj in range(len(power_index)):\n",
    "            part_data = cur_data[:,power_index[jj]]\n",
    "            equal_0 = np.where(part_data == 0)[0]\n",
    "            part_data = np.delete(part_data, equal_0, 0)\n",
    "            if len(part_data) == 0:\n",
    "                cur_data_fea.extend(np.zeros(feature_number))\n",
    "            else:\n",
    "                new_part_data = np.sign(part_data) * np.sqrt(np.abs(part_data))\n",
    "                new_part_data = np.array(new_part_data)\n",
    "                #new_part_data = np.power(part_data,0.5)\n",
    "                #new_part_data = np.log(part_data)\n",
    "                cur_data_fea.extend(feature_mean_and_std(new_part_data))\n",
    "            cur_data_fea.append(len(equal_0) * 1.0 / (maxLen - st))\n",
    "        new_item_index_feature.append(cur_data_fea)\n",
    "    feature = np.array(new_item_index_feature)\n",
    "    print feature.shape\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46620, 21)\n"
     ]
    }
   ],
   "source": [
    "train_fea_new_item_power = new_item_index(clean_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87139, 21)\n"
     ]
    }
   ],
   "source": [
    "test_fea_new_item_power = new_item_index(clean_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "## 检测是否存在nan\n",
    "print np.isnan(np.sum(train_fea_new_item_power))\n",
    "print np.isnan(np.sum(test_fea_new_item_power))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征拼接，得到166维特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fea_item65 = train_fea_item65.reshape(-1,1)\n",
    "test_fea_item65 = test_fea_item65.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46620, 144) (46620, 1) (46620, 21)\n"
     ]
    }
   ],
   "source": [
    "print trainX.shape,train_fea_item65.shape,train_fea_new_item_power.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.concatenate((trainX,train_fea_item65,train_fea_new_item_power),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = np.concatenate((testX,test_fea_item65,test_fea_new_item_power),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46620, 166) (87139, 166)\n"
     ]
    }
   ],
   "source": [
    "print trainX.shape,testX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征preprocessing与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "def data_preprocessing(trainX,testX):\n",
    "    #scaler = preprocessing.RobustScaler(quantile_range=(10.0, 90.0)).fit(trainX)\n",
    "    scaler = preprocessing.StandardScaler().fit(trainX)\n",
    "    scaler.transform(trainX)\n",
    "    scaler.transform(testX)\n",
    "    return trainX,testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,testX = data_preprocessing(trainX,testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    classifiers = {\n",
    "      #'RF': RandomForestClassifier(n_estimators=100, max_depth=10, max_features=0.8,n_jobs=12,verbose=1),  # clf.feature_importances_\n",
    "      'GB': GradientBoostingClassifier(n_estimators=30, learning_rate=0.001, max_depth=5, max_features=0.5,verbose=0,random_state=0),\n",
    "    }\n",
    "    return classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_select_cross_val_score(X,y): ##\n",
    "    classifiers = define_model()\n",
    "    best_clf = classifiers.items()[0]\n",
    "    best_sc = 0.0\n",
    "    for name, clf in classifiers.items():\n",
    "        scores = cross_val_score(clf, X, y, cv=5, scoring='f1',n_jobs=12)\n",
    "        sc_av = scores.mean()\n",
    "        print(name, '\\t--> ', sc_av)\n",
    "        if best_sc < sc_av:\n",
    "            best_sc = sc_av\n",
    "            best_clf = clf\n",
    "    print(best_sc)\n",
    "    return best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GB', '\\t--> ', 0.6507631392285147)\n",
      "0.6507631392285147\n"
     ]
    }
   ],
   "source": [
    "clf = model_select_cross_val_score(trainX,clean_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('f1:', 0.7343659618227392)\n",
      "('acc:', 0.6453882453882454)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(trainX,clean_train_y, test_size=0.5,random_state=1234)\n",
    "clf.fit(X_train, y_train)\n",
    "predict_t = clf.predict(X_test)  ##predict_proba\n",
    "f1_sc = metrics.f1_score(predict_t, y_test)\n",
    "print(\"f1:\",f1_sc)\n",
    "acc = metrics.accuracy_score(predict_t, y_test)\n",
    "print(\"acc:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4053 19257\n"
     ]
    }
   ],
   "source": [
    "print len(clean_train_y[np.where(predict_t == 0)[0]]),len(clean_train_y[np.where(predict_t == 1)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.001, loss='deviance', max_depth=5,\n",
       "              max_features=0.5, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "              presort='auto', random_state=0, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(trainX, clean_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87139,)\n",
      "359 87139 87498 87139\n"
     ]
    }
   ],
   "source": [
    "predict_label = clf.predict(testX)\n",
    "print predict_label.shape\n",
    "print len(test_delete_index),len(testX),len(testFileName),len(clean_test_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 至此，将得到第一阶段的预测结果文件67053.csv，线上F1: 0.67053"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "outPath = tmpFile + '/67053.csv'\n",
    "with open(outPath,\"w\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"id\",\"ret\"])\n",
    "    for index,cur_label in enumerate(predict_label):\n",
    "        writer.writerow([clean_test_name[index],str(cur_label)])\n",
    "    for ii in range(len(test_delete_index)):\n",
    "        index = test_delete_index[ii]\n",
    "        writer.writerow([testFileName[index],'0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stage-2 数据划分--数据提取新特征和accuracy指标训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征4——提取，除了index-8和index-15外的其他index除index-0的向量的均值和方差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = 451\n",
    "feature_number = 2\n",
    "def new_item0(data):  ##power(x,0.5)\n",
    "    new_item_index_feature = []\n",
    "    for ii in range(len(data)):\n",
    "        st = int(data[ii][0][0])\n",
    "        cur_data = data[ii,st:,:]\n",
    "        cur_data_fea = []\n",
    "        feature_number = len(cur_data[0])\n",
    "        speed = []\n",
    "        for index in range(len(cur_data)):\n",
    "            if cur_data[index,0] == 0:\n",
    "                speed.append(cur_data[index,8])\n",
    "            else:\n",
    "                speed.append(cur_data[index,0])\n",
    "        speed = np.array(speed)\n",
    "        if len(np.where(speed == 0)[0]) > 0:\n",
    "            print \"error.\",ii\n",
    "        for jj in range(1,len(cur_data[0])):\n",
    "            if jj == 8 or jj == 15:\n",
    "                continue\n",
    "            part_data = cur_data[:,jj]\n",
    "            new_part_data = part_data / speed\n",
    "            #new_part_data = np.power(part_data,0.5)\n",
    "            #new_part_data = np.log(part_data)\n",
    "            cur_data_fea.extend(feature_mean_and_std(new_part_data))\n",
    "        new_item_index_feature.append(cur_data_fea)\n",
    "    feature = np.array(new_item_index_feature)\n",
    "    print feature.shape\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46620, 138)\n"
     ]
    }
   ],
   "source": [
    "item0_feature = new_item0(clean_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87139, 138)\n"
     ]
    }
   ],
   "source": [
    "item0_feature_test = new_item0(clean_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "## 检测是否存在nan\n",
    "print np.isnan(np.sum(item0_feature))\n",
    "print np.isnan(np.sum(item0_feature_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征5——所有变量的时序特征（fft后的均值和方差）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_feature(sequence_data):\n",
    "    fft_trans = np.abs(np.fft.fft(sequence_data))\n",
    "    dc = fft_trans[0]\n",
    "    freq_spectrum = fft_trans[1:int(np.floor(len(sequence_data) * 1.0 / 2)) + 1]\n",
    "    _freq_sum_ = np.sum(freq_spectrum)\n",
    "    time_feature = []\n",
    "    time_feature.append(np.mean(freq_spectrum))\n",
    "    time_feature.append(np.std(freq_spectrum))\n",
    "    return time_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = 451\n",
    "feature_number = 2\n",
    "def get_time_feature(data):  \n",
    "    new_item_index_feature = []\n",
    "    #feature_index = [0,1,2,3,4,8,16,18,27,28]\n",
    "    feature_index = range(0,72)\n",
    "    for ii in range(len(data)):\n",
    "        st = int(data[ii][0][0])\n",
    "        cur_data = data[ii,st:,:]\n",
    "        cur_data_fea = []\n",
    "        for jj in range(len(feature_index)):\n",
    "            sequence_data = cur_data[:,feature_index[jj]]\n",
    "            cur_data_fea.extend(time_feature(sequence_data))\n",
    "        new_item_index_feature.append(cur_data_fea)\n",
    "    feature = np.array(new_item_index_feature)\n",
    "    print feature.shape\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python2.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/usr/local/lib/python2.7/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/usr/local/lib/python2.7/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/usr/local/lib/python2.7/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46620, 144)\n"
     ]
    }
   ],
   "source": [
    "train_time_fea = get_time_feature(clean_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print np.isnan(np.sum(train_time_fea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/pandas/_libs/__init__.py:4: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime\n",
      "/usr/local/lib/python2.7/site-packages/pandas/__init__.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import (hashtable as _hashtable,\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/dtypes/common.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import algos, lib\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/util/hashing.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import hashing, tslib\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/indexes/base.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import (lib, index as libindex, tslib as libts,\n",
      "/usr/local/lib/python2.7/site-packages/pandas/tseries/offsets.py:21: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  import pandas._libs.tslibs.offsets as liboffsets\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/ops.py:16: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import algos as libalgos, ops as libops\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/indexes/interval.py:32: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs.interval import (\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/internals.py:14: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import internals as libinternals\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/sparse/array.py:33: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  import pandas._libs.sparse as splib\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/window.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  import pandas._libs.window as _window\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/groupby/groupby.py:68: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import (lib, reduction,\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/reshape/reshape.py:30: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import algos as _algos, reshape as _reshape\n",
      "/usr/local/lib/python2.7/site-packages/pandas/io/parsers.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  import pandas._libs.parsers as parsers\n",
      "/usr/local/lib/python2.7/site-packages/pandas/io/pytables.py:50: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from pandas._libs import algos, lib, writers as libwriters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_nan(data):\n",
    "    df = DataFrame(data)\n",
    "    print np.any(df.isnull()) == True\n",
    "    #df = df.fillna(value=0) \n",
    "    df = df.fillna(df.mean())\n",
    "    print np.any(df.isnull()) == True\n",
    "    data = np.array(df)\n",
    "    print np.isnan(np.sum(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "train_time_fea = deal_nan(train_time_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87139, 144)\n"
     ]
    }
   ],
   "source": [
    "test_time_fea = get_time_feature(clean_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "test_time_fea = deal_nan(test_time_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "## 检测是否存在nan\n",
    "print np.isnan(np.sum(item0_feature))\n",
    "print np.isnan(np.sum(item0_feature_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征拼接，得到448维特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.concatenate((trainX,item0_feature,train_time_fea),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = np.concatenate((testX,item0_feature_test,test_time_fea),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46620, 448) (87139, 448)\n"
     ]
    }
   ],
   "source": [
    "print trainX.shape,testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(tmpFile + '/train_feature448.npy',trainX)\n",
    "np.save(tmpFile + '/test_feature448.npy',testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据清洗，删除train，test不同分布的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fea = np.load(tmpFile + '/train_feature448.npy')\n",
    "test_fea = np.load(tmpFile + '/test_feature448.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11412\n",
      "10524 0.22574002574\n"
     ]
    }
   ],
   "source": [
    "del_index_item16 = np.where((train_fea[:,16 + 72] != 0))[0]\n",
    "del_index_item55 = np.where((train_fea[:,55] != 4))[0]\n",
    "del_index_item00 = np.where((train_fea[:,0] < 0))[0]\n",
    "del_i = []\n",
    "del_i.extend(del_index_item16)\n",
    "del_i.extend(del_index_item55)\n",
    "del_i.extend(del_index_item00)\n",
    "print len(del_i)\n",
    "del_i = np.unique(np.array(del_i))\n",
    "print len(del_i), len(del_i) * 1.0 / len(train_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5820\n",
      "5014 0.0575402517816\n"
     ]
    }
   ],
   "source": [
    "del_index_item0 = np.where((test_fea[:,16 + 72] != 0))[0]\n",
    "del_index_item1 = np.where((test_fea[:,55] != 4))[0]\n",
    "del_index_item2 = np.where((test_fea[:,0] < 0))[0]\n",
    "del_j = []\n",
    "del_j.extend(del_index_item0)\n",
    "del_j.extend(del_index_item1)\n",
    "del_j.extend(del_index_item2)\n",
    "print len(del_j)\n",
    "del_j = np.unique(np.array(del_j))\n",
    "print len(del_j), len(del_j) * 1.0 / len(test_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23030 23590\n"
     ]
    }
   ],
   "source": [
    "print len(np.where(clean_train_y == 0)[0]),len(np.where(clean_train_y == 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4535 5989\n"
     ]
    }
   ],
   "source": [
    "print len(np.where(clean_train_y[del_i] == 0)[0]),len(np.where(clean_train_y[del_i] == 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fea_clean = np.delete(train_fea,del_i,axis=0)\n",
    "train_y_clean = np.delete(clean_train_y,del_i,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36096, 448) (36096,)\n"
     ]
    }
   ],
   "source": [
    "print train_fea_clean.shape,train_y_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82125, 448)\n"
     ]
    }
   ],
   "source": [
    "test_fea_clean = np.delete(test_fea,del_j,axis=0)\n",
    "print test_fea_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82125\n"
     ]
    }
   ],
   "source": [
    "clean_test_name2 = []\n",
    "for ii in range(len(clean_test_name)):\n",
    "    if ii not in del_j:\n",
    "        clean_test_name2.append(clean_test_name[ii])\n",
    "print len(clean_test_name2)\n",
    "clean_test_name = clean_test_name2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据划分--36part，根据index-53,index-54,index65,index-16，分别代表机器的不同状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "data_split_train_index = []\n",
    "data_split_test_index = []\n",
    "item53 = (6,3,2)\n",
    "item54 = (6,4)\n",
    "for ii in range(1,4):  ##16\n",
    "    for jj in range(0,2): ##65,\n",
    "        for kk in range(len(item53)): ##53\n",
    "            for LL in range(len(item54)):##54\n",
    "                if jj == 0:\n",
    "                    index = np.where((train_fea_clean[:,16] == ii) & (train_fea_clean[:,65] < 300) \n",
    "                                     & (train_fea_clean[:,53] == item53[kk]) &\n",
    "                                       (train_fea_clean[:,54] == item54[LL]))[0]\n",
    "                else:\n",
    "                    index = np.where((train_fea_clean[:,16] == ii) & (train_fea_clean[:,65] >= 300) \n",
    "                                     & (train_fea_clean[:,53] == item53[kk]) &\n",
    "                                       (train_fea_clean[:,54] == item54[LL]))[0]\n",
    "                data_split_train_index.append(index)\n",
    "                if len(index) == 0:\n",
    "                    pass\n",
    "                    #print ii,jj,item53[kk],item54[LL]\n",
    "                if jj == 0:\n",
    "                    index_test = np.where((test_fea_clean[:,16] == ii) & (test_fea_clean[:,65] < 300) \n",
    "                                     & (test_fea_clean[:,53] == item53[kk]) &\n",
    "                                       (test_fea_clean[:,54] == item54[LL]))[0]\n",
    "                else:\n",
    "                    index_test = np.where((test_fea_clean[:,16] == ii) & (test_fea_clean[:,65] >= 300) \n",
    "                                     & (test_fea_clean[:,53] == item53[kk]) &\n",
    "                                       (test_fea_clean[:,54] == item54[LL]))[0]\n",
    "                data_split_test_index.append(index_test)\n",
    "                \n",
    "print len(data_split_train_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查看每个划分下的数据情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 18665\n",
      "3 2764\n",
      "5 781\n",
      "6 3847\n",
      "9 302\n",
      "11 37\n",
      "12 94\n",
      "15 832\n",
      "17 2382\n",
      "21 62\n",
      "23 148\n",
      "24 39356\n",
      "27 2920\n",
      "29 958\n",
      "30 4937\n",
      "33 159\n",
      "35 119\n",
      "78363 0.954191780822\n"
     ]
    }
   ],
   "source": [
    "sumV = 0\n",
    "for ii in range(len(data_split_test_index)):\n",
    "    if len(data_split_test_index[ii]) < 10:\n",
    "        continue\n",
    "    print ii,len(data_split_test_index[ii])\n",
    "    sumV += len(data_split_test_index[ii])\n",
    "print sumV,sumV * 1.0 / len(test_fea_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18665 0 7227 4240 2987\n",
      "2764 3 1094 726 368\n",
      "781 5 585 215 370\n",
      "3847 6 8197 4293 3904\n",
      "302 9 1016 640 376\n",
      "37 11 172 112 60\n",
      "94 12 48 36 12\n",
      "832 15 295 177 118\n",
      "2382 17 268 199 69\n",
      "62 21 100 33 67\n",
      "39356 24 11712 5201 6511\n",
      "2920 27 873 370 503\n",
      "958 29 195 150 45\n",
      "4937 30 1844 784 1060\n",
      "159 33 92 26 66\n",
      "33718 0.934120124113\n"
     ]
    }
   ],
   "source": [
    "sumV = 0\n",
    "for ii in range(len(data_split_train_index)):\n",
    "    cur_part = data_split_train_index[ii]\n",
    "    if len(cur_part) < 10:\n",
    "        continue\n",
    "    print len(data_split_test_index[ii]),ii,len(cur_part),len(np.where(train_y_clean[cur_part] == 0)[0]),len(np.where(train_y_clean[cur_part] == 1)[0])\n",
    "    sumV += len(data_split_train_index[ii])\n",
    "print sumV,sumV * 1.0 / len(train_fea_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从上述结果，我们下面着重分析子part0和part24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 子part24，train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11712, 448) (11712,)\n"
     ]
    }
   ],
   "source": [
    "index = 24\n",
    "partIndex = data_split_train_index[index]\n",
    "train_fea_clean_part = train_fea_clean[partIndex]\n",
    "train_y_clean_part = train_y_clean[partIndex]\n",
    "print train_fea_clean_part.shape,train_y_clean_part.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39356, 448)\n"
     ]
    }
   ],
   "source": [
    "test_index_part = data_split_test_index[index]\n",
    "test_fea_clean_part = test_fea_clean[test_index_part]\n",
    "print test_fea_clean_part.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_y_clean_part,test_fea_clean_part = data_preprocessing(train_fea_clean_part,test_fea_clean_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_select_cross_val_score(X,y): ##\n",
    "    classifiers = define_model()\n",
    "    best_clf = classifiers.items()[0]\n",
    "    best_sc = 0.0\n",
    "    for name, clf in classifiers.items():\n",
    "        scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy',n_jobs=12)  ##accuracy,f1_weighted\n",
    "        sc_av = scores.mean()\n",
    "        print scores\n",
    "        print scores.std()\n",
    "        print(name, '\\t--> ', sc_av)\n",
    "        if best_sc < sc_av:\n",
    "            best_sc = sc_av\n",
    "            best_clf = clf\n",
    "    print(best_sc)\n",
    "    return best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    classifiers = {\n",
    "      'RF': RandomForestClassifier(n_estimators=30, max_depth=4, max_features=0.5,n_jobs=12,verbose=1,\n",
    "                                   random_state=1234,class_weight={0:1,1:2}),  # clf.feature_importances_\n",
    "    }\n",
    "    return classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:   13.9s finished\n",
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:   13.8s finished\n",
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:   13.7s finished\n",
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:   13.4s finished\n",
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:   12.6s finished\n",
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55375427 0.6118702  0.58625107 0.53928266 0.70025619]\n",
      "0.05690651769978695\n",
      "('RF', '\\t--> ', 0.5982828771535079)\n",
      "0.5982828771535079\n"
     ]
    }
   ],
   "source": [
    "clf = model_select_cross_val_score(train_fea_clean_part,train_y_clean_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_fea_clean_part,train_y_clean_part,test_size=0.2,random_state=1234)\n",
    "clf.fit(X_train, y_train)\n",
    "predict_t = clf.predict(X_test)  ##predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405 1938\n"
     ]
    }
   ],
   "source": [
    "print len(np.where(predict_t == 0)[0]),len(np.where(predict_t == 1)[0])\n",
    "mid_index = len(np.where(predict_t == 0)[0]) // 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86537439 0.13462561]\n",
      "('acc-label0:', 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "predict_t_c = clf.predict_proba(X_test)  ##predict_proba\n",
    "predict_t_xx = np.ones(len(predict_t_c))\n",
    "sort_index_predict = np.argsort(-predict_t_c[:,0])[:mid_index]   ##等于上述1764的一半，然后用这个概率卡test集的阈值\n",
    "#print predict_t_c[sort_index_predict][:,0],y_test[sort_index_predict]\n",
    "print predict_t_c[sort_index_predict[-1]]\n",
    "predict_t_xx[sort_index_predict] = 0\n",
    "acc = metrics.accuracy_score(predict_t_xx[sort_index_predict], y_test[sort_index_predict])\n",
    "print(\"acc-label0:\",acc)\n",
    "threshold = predict_t_c[sort_index_predict[-1]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:    3.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight={0: 1, 1: 2},\n",
       "            criterion='gini', max_depth=4, max_features=0.5,\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=30, n_jobs=12, oob_score=False, random_state=1234,\n",
       "            verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_fea_clean_part,train_y_clean_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39356, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "part_predict_proba = clf.predict_proba(test_fea_clean_part) \n",
    "print part_predict_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sort = np.argsort(-part_predict_proba[:,0])[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9958094501346794 0.7558864903915677\n"
     ]
    }
   ],
   "source": [
    "print np.max(part_predict_proba[score_sort,0]),np.min(part_predict_proba[score_sort,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39356\n"
     ]
    }
   ],
   "source": [
    "part_predict = np.ones(len(part_predict_proba))\n",
    "print len(part_predict)\n",
    "part_predict[score_sort] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 37356\n"
     ]
    }
   ],
   "source": [
    "print len(np.where(part_predict == 0)[0]),len(np.where(part_predict == 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39356,)\n",
      "82125\n",
      "(39356, 2)\n"
     ]
    }
   ],
   "source": [
    "print test_index_part.shape\n",
    "print len(clean_test_name)\n",
    "print part_predict_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "mm = {}\n",
    "for ii in range(len(test_index_part)):\n",
    "    if part_predict[ii] == 0:\n",
    "        label0_name = clean_test_name[test_index_part[ii]]\n",
    "        mm[label0_name] = part_predict_proba[ii][0]\n",
    "print len(mm.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 至此,得到67234.csv,线上分数0.67234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "with open(tmpFile + '/67234.csv','w') as ff:\n",
    "    with open(tmpFile + '/67053.csv') as readLine:\n",
    "        for indextt,line in enumerate(readLine):\n",
    "            if indextt == 0:\n",
    "                ff.write(line)\n",
    "            else:\n",
    "                tt = line.strip().split(',')\n",
    "                if int(tt[1]) == 0:\n",
    "                    ff.write(tt[0] + \",0\\n\")\n",
    "                else:\n",
    "                    if mm.has_key(tt[0]):\n",
    "                        cnt += 1\n",
    "                        ff.write(tt[0] + \",0\\n\")\n",
    "                    else:\n",
    "                        ff.write(tt[0] + \",1\\n\")\n",
    "print cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 子part0，train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227, 448) (7227,)\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "partIndex = data_split_train_index[index]\n",
    "train_fea_clean_part = train_fea_clean[partIndex]\n",
    "train_y_clean_part = train_y_clean[partIndex]\n",
    "print train_fea_clean_part.shape,train_y_clean_part.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18665, 448)\n"
     ]
    }
   ],
   "source": [
    "test_index_part = data_split_test_index[index]\n",
    "test_fea_clean_part = test_fea_clean[test_index_part]\n",
    "print test_fea_clean_part.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=30, max_depth=5, max_features=0.5,n_jobs=12,verbose=1,\n",
    "                                   random_state=1234,class_weight={0:1,1:2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "#clf = GradientBoostingClassifier(n_estimators=30, learning_rate=0.001,max_depth=5,max_features=1.0,verbose=1,random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_fea_clean_part,train_y_clean_part,test_size=0.5,random_state=1234)\n",
    "clf.fit(X_train, y_train)\n",
    "predict_t = clf.predict(X_test)  ##predict_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1805 1809\n"
     ]
    }
   ],
   "source": [
    "print len(np.where(predict_t == 0)[0]),len(np.where(predict_t == 1)[0])\n",
    "mid_index = len(np.where(predict_t == 0)[0]) // 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.94095847 0.05904153]\n",
      "('acc-label0:', 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "predict_t_c = clf.predict_proba(X_test)  ##predict_proba\n",
    "predict_t_xx = np.ones(len(predict_t_c))\n",
    "sort_index_predict = np.argsort(-predict_t_c[:,0])[:mid_index]   ##等于上述1764的一半，然后用这个概率卡test集的阈值\n",
    "#print predict_t_c[sort_index_predict][:,0],y_test[sort_index_predict]\n",
    "print predict_t_c[sort_index_predict[-1]]\n",
    "predict_t_xx[sort_index_predict] = 0\n",
    "acc = metrics.accuracy_score(predict_t_xx[sort_index_predict], y_test[sort_index_predict])\n",
    "print(\"acc-label0:\",acc)\n",
    "threshold = predict_t_c[sort_index_predict[-1]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:    2.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight={0: 1, 1: 2},\n",
       "            criterion='gini', max_depth=5, max_features=0.5,\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=30, n_jobs=12, oob_score=False, random_state=1234,\n",
       "            verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_fea_clean_part,train_y_clean_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  30 out of  30 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18665, 2)\n"
     ]
    }
   ],
   "source": [
    "part_predict_proba = clf.predict_proba(test_fea_clean_part) \n",
    "print part_predict_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sort = np.argsort(-part_predict_proba[:,0])[:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9731655332587609 0.8768205959068534\n"
     ]
    }
   ],
   "source": [
    "print np.max(part_predict_proba[score_sort,0]),np.min(part_predict_proba[score_sort,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18665\n"
     ]
    }
   ],
   "source": [
    "part_predict = np.ones(len(part_predict_proba))\n",
    "print len(part_predict)\n",
    "part_predict[score_sort] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 16165\n"
     ]
    }
   ],
   "source": [
    "print len(np.where(part_predict == 0)[0]),len(np.where(part_predict == 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "mm = {}\n",
    "for ii in range(len(test_index_part)):\n",
    "    if part_predict[ii] == 0:\n",
    "        label0_name = clean_test_name[test_index_part[ii]]\n",
    "        mm[label0_name] = part_predict_proba[ii][0]\n",
    "print len(mm.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 至此,得到67835.csv,线上分数0.67835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2484\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "with open(tmpFile + '/67835.csv','w') as ff:\n",
    "    with open(tmpFile + '/67234.csv') as readLine:\n",
    "        for indextt,line in enumerate(readLine):\n",
    "            if indextt == 0:\n",
    "                ff.write(line)\n",
    "            else:\n",
    "                tt = line.strip().split(',')\n",
    "                if int(tt[1]) == 0:\n",
    "                    ff.write(tt[0] + \",0\\n\")\n",
    "                else:\n",
    "                    if mm.has_key(tt[0]):\n",
    "                        cnt += 1\n",
    "                        ff.write(tt[0] + \",0\\n\")\n",
    "                    else:\n",
    "                        ff.write(tt[0] + \",1\\n\")\n",
    "print cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
